{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "from deepctr.feature_column import SparseFeat, VarLenSparseFeat, VarUserLenSparseFeat, DenseFeat,get_feature_names\n",
    "from deepctr.models import MIN\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('samples.pkl', 'rb') as f: \n",
    "    train_data = pickle.load(f)\n",
    "    test_data = pickle.load(f)\n",
    "    user_count, item_count, cate_count = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train_data)\n",
    "random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)+len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "DeepCTR version 0.9.1 detected. Your version is 0.9.0.\n",
      "Use `pip install -U deepctr` to upgrade.Changelog: https://github.com/shenweichen/DeepCTR/releases/tag/v0.9.1\n"
     ]
    }
   ],
   "source": [
    "#(uid, mid, cat, histlen, mid_list, cat_list, mid_neg_list, cat_neg_list, item_uid_list, item_midlen_list,\\\n",
    "                     #item_mid_list, item_cat_list, item_histlen, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfixedseq(data, fixedlen, ulen):\n",
    "    ts=data#(uid, mid, cat, histlen, mid_list, cat_list, mid_neg_list, cat_neg_list, users_list, ulen_list, user_item_list, user_cat_list,label)\n",
    "    counts = len(ts)\n",
    "\n",
    "    u, i, c, sl, y = [], [], [], [], []\n",
    "    hist_i = []\n",
    "    hist_c = []\n",
    "    #user_u = []\n",
    "    user_sl = []\n",
    "    user_i = []\n",
    "    user_c = []\n",
    "    user_n = []\n",
    "    \n",
    "\n",
    "    for t in ts:\n",
    "        u.append(t[0])\n",
    "        i.append(t[1])\n",
    "        c.append(t[2])\n",
    "        sl.append(t[3])\n",
    "        hist_i.append(t[4])\n",
    "        hist_c.append(t[5])\n",
    "        #user_u.append(t[8])\n",
    "        user_sl.append(t[9])\n",
    "        user_n.append(t[12])\n",
    "        y.append(t[13])\n",
    "        \n",
    "    hist_i = pad_sequences(hist_i, maxlen=fixedlen, padding='post', truncating='post')\n",
    "    hist_c = pad_sequences(hist_c, maxlen=fixedlen, padding='post', truncating='post')\n",
    "    user_sl = pad_sequences(user_sl, maxlen=ulen, padding='post', truncating='post')\n",
    "    user_sl = np.expand_dims(user_sl,axis=2)\n",
    "    \n",
    "    user_i_tmp = []\n",
    "    user_c_tmp = []\n",
    "    for t in ts:\n",
    "        n = t[12]\n",
    "        user_i_tmp = t[10] \n",
    "        user_i_tmp = pad_sequences(user_i_tmp, maxlen=fixedlen, padding='post', truncating='post')\n",
    "        user_c_tmp = t[11] \n",
    "        user_c_tmp = pad_sequences(user_c_tmp, maxlen=fixedlen, padding='post', truncating='post')\n",
    "        \n",
    "        if n>ulen or n==ulen:\n",
    "            user_i_tmp = user_i_tmp[:ulen]\n",
    "            user_c_tmp = user_c_tmp[:ulen]\n",
    "        else:\n",
    "            ns = ulen-n\n",
    "            zero = np.zeros([ns, fixedlen], np.int64)\n",
    "            user_i_tmp = np.row_stack((user_i_tmp,zero))\n",
    "            user_c_tmp = np.row_stack((user_c_tmp,zero))\n",
    "            \n",
    "        user_i.append(user_i_tmp)\n",
    "        user_c.append(user_c_tmp)\n",
    "\n",
    "    return u, i, c, sl, hist_i, hist_c, user_sl, user_i, user_c, user_n, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getfixedseq_short(data, fixedlen, ulen):\n",
    "    ts=data\n",
    "    counts = len(ts)\n",
    "\n",
    "    u, i, c, sl, y = [], [], [], [], []\n",
    "    hist_i = []\n",
    "    hist_c = []\n",
    "    #user_u = []\n",
    "    user_sl = []\n",
    "    user_i = []\n",
    "    user_c = []\n",
    "    user_n = []\n",
    "    \n",
    "\n",
    "    for t in ts:\n",
    "        u.append(t[0])\n",
    "        i.append(t[1])\n",
    "        c.append(t[2])\n",
    "        sl.append(t[3])\n",
    "        hist_i.append(t[4])\n",
    "        hist_c.append(t[5])\n",
    "        #user_u.append(t[8])\n",
    "        user_sl.append(t[9])\n",
    "        user_n.append(t[12])\n",
    "        y.append(t[13])\n",
    "        \n",
    "    hist_i = pad_sequences(hist_i, maxlen=fixedlen, padding='pre', truncating='pre')## 从后向前截取，向前补零\n",
    "    hist_c = pad_sequences(hist_c, maxlen=fixedlen, padding='pre', truncating='pre')\n",
    "    user_sl = pad_sequences(user_sl, maxlen=ulen, padding='post', truncating='post')\n",
    "    user_sl = np.expand_dims(user_sl,axis=2)\n",
    "    \n",
    "    user_i_tmp = []\n",
    "    user_c_tmp = []\n",
    "    for t in ts:\n",
    "        n = t[12]\n",
    "        user_i_tmp = t[10] \n",
    "        user_i_tmp = pad_sequences(user_i_tmp, maxlen=fixedlen, padding='pre', truncating='pre')\n",
    "        user_c_tmp = t[11] \n",
    "        user_c_tmp = pad_sequences(user_c_tmp, maxlen=fixedlen, padding='pre', truncating='pre')\n",
    "        \n",
    "        if n>ulen or n==ulen:\n",
    "            user_i_tmp = user_i_tmp[:ulen]\n",
    "            user_c_tmp = user_c_tmp[:ulen]\n",
    "        else:\n",
    "            ns = ulen-n\n",
    "            zero = np.zeros([ns, fixedlen], np.int64)\n",
    "            user_i_tmp = np.row_stack((user_i_tmp,zero))\n",
    "            user_c_tmp = np.row_stack((user_c_tmp,zero))\n",
    "            \n",
    "        user_i.append(user_i_tmp)\n",
    "        user_c.append(user_c_tmp)\n",
    "\n",
    "    return u, i, c, sl, hist_i, hist_c, user_sl, user_i, user_c, user_n, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xy_fd(data, fixedlen, ulen, isshort=False, hash_flag=False):\n",
    "    \n",
    "    feature_columns = [SparseFeat('user', user_count, embedding_dim=12, use_hash=hash_flag),\n",
    "                       SparseFeat('item_id', item_count+1, embedding_dim=8, use_hash=hash_flag),\n",
    "                       SparseFeat('cate_id', cate_count+1, embedding_dim=4, use_hash=hash_flag)]\n",
    "    \n",
    "    feature_columns += [\n",
    "        VarLenSparseFeat(SparseFeat('hist_item_id', vocabulary_size=item_count+1, embedding_dim=8, embedding_name='item_id'),\n",
    "                         maxlen=fixedlen, length_name=\"seq_length\"),\n",
    "        VarLenSparseFeat(SparseFeat('hist_cate_id', cate_count+1, embedding_dim=4, embedding_name='cate_id'), maxlen=fixedlen,\n",
    "                         length_name=\"seq_length\")]\n",
    "    \n",
    "    feature_columns += [\n",
    "        VarUserLenSparseFeat(SparseFeat('users_item_id', vocabulary_size=item_count+1, embedding_dim=8, embedding_name='item_id'),\n",
    "                         maxuser=ulen, maxlen=fixedlen, user_length_name=\"user_length\", length_name=\"user_seq_length\"),\n",
    "        VarUserLenSparseFeat(SparseFeat('users_cate_id', cate_count+1, embedding_dim=4, embedding_name='cate_id'), maxuser=ulen,\n",
    "                        maxlen=fixedlen,user_length_name=\"user_length\", length_name=\"user_seq_length\")]\n",
    "    \n",
    "        \n",
    "    # Notice: History behavior sequence feature name must start with \"hist_\".\n",
    "    behavior_feature_list = [\"item_id\", \"cate_id\"]\n",
    "    \n",
    "    if isshort:\n",
    "        u, i, c, sl, hist_i, hist_c, user_sl, user_i, user_c, user_n, y = getfixedseq_short(data,fixedlen,ulen)\n",
    "        print(\"short time\")\n",
    "    else:\n",
    "        u, i, c, sl, hist_i, hist_c, user_sl, user_i, user_c, user_n, y = getfixedseq(data,fixedlen,ulen)\n",
    "        print(\"long time\")\n",
    "        \n",
    "    \n",
    "    uid = np.array(u)\n",
    "    iid = np.array(i)  # 0 is mask value\n",
    "    cate_id = np.array(c)  # 0 is mask value\n",
    "\n",
    "    hist_iid = np.array(hist_i)\n",
    "    hist_cate_id = np.array(hist_c)\n",
    "    seq_length = np.array(sl)\n",
    "    \n",
    "    user_hist_iid = np.array(user_i)\n",
    "    user_hist_cate_id = np.array(user_c)\n",
    "    user_seq_length = np.array(user_sl)\n",
    "    user_length = np.array(user_n)\n",
    "\n",
    "    feature_dict = {'user': uid, 'item_id': iid, 'cate_id': cate_id,\n",
    "                    'hist_item_id': hist_iid, 'hist_cate_id': hist_cate_id, \"seq_length\": seq_length,\n",
    "                   'users_item_id':user_hist_iid, 'users_cate_id':user_hist_cate_id, \"user_seq_length\":user_seq_length,\n",
    "                   \"user_length\":user_length}\n",
    "    \n",
    "    x = {name: feature_dict[name] for name in get_feature_names(feature_columns)}\n",
    "    y = np.array(y)\n",
    "    return x, y, feature_columns, behavior_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dhu133/anaconda3/envs/tensorflow2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dhu133/anaconda3/envs/tensorflow2.0/lib/python3.7/site-packages/tensorflow_core/python/keras/initializers.py:143: calling RandomNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "short time\n",
      "short time\n",
      "WARNING:tensorflow:From /home/dhu133/anaconda3/envs/tensorflow2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dhu133/anaconda3/envs/tensorflow2.0/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1280 samples, validate on 320 samples\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.69154, saving model to ./result/grocery/0-weights-best.tflite\n",
      "1280/1280 - 8s - loss: 0.7866 - binary_crossentropy: 0.7866 - val_loss: 0.6915 - val_binary_crossentropy: 0.6915\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.69154\n",
      "1280/1280 - 1s - loss: 0.7056 - binary_crossentropy: 0.7056 - val_loss: 0.6944 - val_binary_crossentropy: 0.6944\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.69154\n",
      "1280/1280 - 1s - loss: 0.6615 - binary_crossentropy: 0.6615 - val_loss: 0.7267 - val_binary_crossentropy: 0.7267\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.69154\n",
      "1280/1280 - 1s - loss: 0.6084 - binary_crossentropy: 0.6084 - val_loss: 0.7452 - val_binary_crossentropy: 0.7452\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.69154\n",
      "1280/1280 - 1s - loss: 0.4914 - binary_crossentropy: 0.4914 - val_loss: 0.7646 - val_binary_crossentropy: 0.7646\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.69154\n",
      "1280/1280 - 1s - loss: 0.3208 - binary_crossentropy: 0.3208 - val_loss: 0.9021 - val_binary_crossentropy: 0.9021\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.69154\n",
      "1280/1280 - 1s - loss: 0.2309 - binary_crossentropy: 0.2309 - val_loss: 1.1440 - val_binary_crossentropy: 1.1440\n",
      "Epoch 8/10\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.69154\n",
      "1280/1280 - 1s - loss: 0.1784 - binary_crossentropy: 0.1784 - val_loss: 1.1012 - val_binary_crossentropy: 1.1011\n",
      "Epoch 9/10\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.69154\n",
      "1280/1280 - 1s - loss: 0.1487 - binary_crossentropy: 0.1487 - val_loss: 1.2237 - val_binary_crossentropy: 1.2237\n",
      "Epoch 10/10\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.69154\n",
      "1280/1280 - 1s - loss: 0.1183 - binary_crossentropy: 0.1183 - val_loss: 1.2305 - val_binary_crossentropy: 1.2305\n",
      "Test results of the 0 training:\n",
      "test LogLoss 0.68622\n",
      "test AUC 0.54\n",
      "loglosslist: [0.68622]\n",
      "auclist: [0.54]\n",
      "Average best LogLoss: 0.6862\n",
      "Average best AUC: 0.54\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if tf.__version__ >= '2.0.0':\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "    \n",
    "    auclist = []\n",
    "    loglosslist = []\n",
    "    repeats = 1\n",
    "    fixedlen = 15\n",
    "    ulen =10\n",
    "    IsShort = True\n",
    "    \n",
    "    trainx, trainy, feature_columns, behavior_feature_list = get_xy_fd(train_data, fixedlen, ulen,isshort=IsShort)##默认long time,isshort= False\n",
    "    testx, testy, feature_columns, behavior_feature_list = get_xy_fd(test_data, fixedlen, ulen,isshort=IsShort)\n",
    "    \n",
    "    \n",
    "    for i in range(repeats):\n",
    "\n",
    "        model = MIN(feature_columns, behavior_feature_list,itemshort=IsShort, att_head_num=1,dnn_hidden_units=(128, 64),task='binary')\n",
    "        model.compile('adam', 'binary_crossentropy',\n",
    "                  metrics=['binary_crossentropy'])\n",
    "\n",
    "        filepath=\"./result/grocery/\"+str(i)+\"-weights-best.tflite\"\n",
    "        checkpointer = ModelCheckpoint(filepath, monitor='val_loss',verbose=1, save_best_only=True,mode='min',save_weights_only=True)\n",
    "        callbacks_list= [checkpointer]\n",
    "        \n",
    "        #训练\n",
    "        history = model.fit(trainx, trainy, batch_size=128, epochs=10, verbose=2, validation_split=0.2,callbacks=callbacks_list,shuffle=True)\n",
    "        \n",
    "        #测试\n",
    "        loadfile = \"./result/grocery/\"+str(i)+ \"-weights-best.tflite\"\n",
    "        model.load_weights(loadfile)\n",
    "        pred_ans = model.predict(testx, batch_size=256)\n",
    "        logloss = round(log_loss(testy, pred_ans,eps = 1e-7),5)\n",
    "        auc = round(roc_auc_score(testy, pred_ans),5)\n",
    "        \n",
    "        loglosslist.append(logloss)\n",
    "        auclist.append(auc)\n",
    "         \n",
    "        print(\"Test results of the \"+str(i)+\" training:\")\n",
    "        print(\"test LogLoss\", logloss)\n",
    "        print(\"test AUC\", auc)\n",
    "    \n",
    "    print(\"loglosslist:\",loglosslist)\n",
    "    print(\"auclist:\",auclist)\n",
    "    print(\"Average best LogLoss:\", round(np.mean(loglosslist),4))\n",
    "    print(\"Average best AUC:\", round(np.mean(auclist),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
