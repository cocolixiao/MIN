{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../process_data.py meta_Grocery.json reviews_Grocery_5.json #暂不执行split_test()---20180118/20190119"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成用户序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import hashlib\n",
    "import random\n",
    "\n",
    "fin = open(\"jointed-new\", \"r\")\n",
    "fo = open(\"jointed-new-userseq\", \"w\")\n",
    "\n",
    "last_user = \"0\"\n",
    "common_fea = \"\"\n",
    "line_idx = 0\n",
    "for line in fin:\n",
    "    items = line.strip().split(\"\\t\")\n",
    "    clk = int(items[0])\n",
    "    user = items[1]\n",
    "    movie_id = items[2]\n",
    "    dt = items[4]\n",
    "    cat1 = items[5]\n",
    "\n",
    "    if user != last_user:\n",
    "        movie_id_list = []\n",
    "        cate1_list = []\n",
    "        #print >> fo, items[1] + \"\\t\" + user + \"\\t\" + movie_id + \"\\t\" + cat1 +\"\\t\" + \"\" + \"\\t\" + \"\" \n",
    "    else:\n",
    "        history_clk_num = len(movie_id_list)\n",
    "        cat_str = \"\"\n",
    "        mid_str = \"\"\n",
    "        for c1 in cate1_list:\n",
    "            cat_str += c1 + \"\u0002\"\n",
    "        for mid in movie_id_list:\n",
    "            mid_str += mid + \"\u0002\"\n",
    "        if len(cat_str) > 0: cat_str = cat_str[:-1]\n",
    "        if len(mid_str) > 0: mid_str = mid_str[:-1]\n",
    "        if history_clk_num >= 1:    # 8 is the average length of user behavior\n",
    "            print(items[0] + \"\\t\" + user + \"\\t\" + movie_id + \"\\t\" + cat1 +\"\\t\" + mid_str + \"\\t\" + cat_str,file=fo)\n",
    "    last_user = user\n",
    "    if clk:\n",
    "        movie_id_list.append(movie_id)\n",
    "        cate1_list.append(cat1)                \n",
    "    line_idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成物品序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import hashlib\n",
    "import random\n",
    "\n",
    "fin = open(\"jointed-new-userseq\", \"r\")\n",
    "ftest = open(\"jointed-new-userseq-itemseq\", \"w\")##\n",
    "\n",
    "item_map = {}##key=item_id(clk=1),value = ()\n",
    "last_user = \"0\"\n",
    "common_fea = \"\"\n",
    "line_idx = 0\n",
    "for line in fin:\n",
    "    items = line.strip().split(\"\\t\")\n",
    "    clk = int(items[0])\n",
    "    user = items[1]\n",
    "    item_id = items[2]\n",
    "    cat1 = items[3]\n",
    "    mid_list = items[4]\n",
    "    cat_list = items[5]\n",
    "    \n",
    "    if clk==1:\n",
    "        if item_id not in item_map:\n",
    "            item_map[item_id]= []\n",
    "        item_map[item_id].append((user,mid_list,cat_list,len(mid_list.split(\"\u0002\"))))\n",
    "        \n",
    "    line_idx += 1\n",
    "    \n",
    "sorted_item_map = {}\n",
    "for key in item_map:\n",
    "    sorted_item_map[key] = sorted(item_map[key], key=lambda x:x[3], reverse=True)#按照长度排序\n",
    "\n",
    "fin = open(\"jointed-new-userseq\", \"r\")    \n",
    "i = 0    \n",
    "for line in fin:\n",
    "    items = line.strip().split(\"\\t\")\n",
    "    clk = int(items[0])\n",
    "    user = items[1]\n",
    "    item_id = items[2]\n",
    "    cat1 = items[3]\n",
    "    mid_list = items[4]\n",
    "    cat_list = items[5]\n",
    "    \n",
    "    history_users_feats = \"\"\n",
    "    history_users_num = 0\n",
    "    if item_id in sorted_item_map:\n",
    "        temp = sorted_item_map[item_id]\n",
    "        for t in temp:\n",
    "            item_bhvs_feat=t[0]+'_'+t[1]+'_'+t[2]\n",
    "            history_users_feats = history_users_feats+\";\"+item_bhvs_feat\n",
    "            history_users_num+=1\n",
    "            \n",
    "    print(items[0] + \"\\t\" + user + \"\\t\" + item_id + \"\\t\" + cat1 +\"\\t\" + mid_list + \"\\t\" + cat_list + \"\\t\" + history_users_feats + \"\\t\" + str(history_users_num),file=ftest)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "f_train = open(\"jointed-new-userseq-itemseq\", \"r\")#根据\"local_train_splitByUser\"和\"local_test_splitByUser\"生成\n",
    "uid_dict = {}\n",
    "mid_dict = {}\n",
    "cat_dict = {}\n",
    "\n",
    "iddd = 0\n",
    "for line in f_train:\n",
    "    arr = line.strip(\"\\n\").split(\"\\t\")\n",
    "    clk = arr[0]\n",
    "    uid = arr[1]\n",
    "    mid = arr[2]\n",
    "    cat = arr[3]\n",
    "    mid_list = arr[4]\n",
    "    cat_list = arr[5]\n",
    "    \n",
    "    history_users_feats = arr[6]\n",
    "    history_users_num = arr[7]\n",
    "    \n",
    "    if uid not in uid_dict:\n",
    "        uid_dict[uid] = 0\n",
    "    uid_dict[uid] += 1\n",
    "    if mid not in mid_dict:\n",
    "        mid_dict[mid] = 0\n",
    "    mid_dict[mid] += 1\n",
    "    if cat not in cat_dict:\n",
    "        cat_dict[cat] = 0\n",
    "    cat_dict[cat] += 1\n",
    "    \n",
    "    if len(mid_list) == 0:\n",
    "        continue\n",
    "    for m in mid_list.split(\"\u0002\"):\n",
    "        if m not in mid_dict:\n",
    "            mid_dict[m] = 0\n",
    "        mid_dict[m] += 1\n",
    "    for c in cat_list.split(\"\u0002\"):\n",
    "        if c not in cat_dict:\n",
    "            cat_dict[c] = 0\n",
    "        cat_dict[c] += 1\n",
    "    \n",
    "    if int(history_users_num) == 0:\n",
    "        continue\n",
    "    for bhvs in history_users_feats.split(\";\")[1:]:\n",
    "        arr = bhvs.split(\"_\")\n",
    "        if arr[1].strip() == \"\":\n",
    "            continue\n",
    "        bhv_uid = arr[0]\n",
    "        bhv_mid_list = arr[1]\n",
    "        bhv_cat_list = arr[2]\n",
    "\n",
    "        if bhv_uid not in uid_dict:\n",
    "            uid_dict[bhv_uid] = 0\n",
    "        uid_dict[bhv_uid] += 1\n",
    "\n",
    "        for bm in bhv_mid_list.split(\"\u0002\"):\n",
    "            if bm not in mid_dict:\n",
    "                mid_dict[bm] = 0\n",
    "            mid_dict[bm] += 1\n",
    "        for bc in bhv_cat_list.split(\"\u0002\"):\n",
    "            if bc not in cat_dict:\n",
    "                cat_dict[bc] = 0\n",
    "            cat_dict[bc] += 1        \n",
    "    iddd+=1\n",
    "\n",
    "#sorted_uid_dict = list(uid_dict.keys())\n",
    "#random.shuffle(sorted_uid_dict)\n",
    "sorted_uid_dict = sorted(uid_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "sorted_mid_dict = sorted(mid_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "sorted_cat_dict = sorted(cat_dict.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "uid_voc = {}\n",
    "index = 0\n",
    "for key, value in sorted_uid_dict:\n",
    "    uid_voc[key] = index\n",
    "    index += 1\n",
    "\n",
    "mid_voc = {}\n",
    "mid_voc[\"default_mid\"] = 1\n",
    "index = 2\n",
    "for key, value in sorted_mid_dict:\n",
    "    mid_voc[key] = index\n",
    "    index += 1\n",
    "\n",
    "cat_voc = {}\n",
    "cat_voc[\"default_cat\"] = 1\n",
    "index = 2\n",
    "for key, value in sorted_cat_dict:\n",
    "    cat_voc[key] = index\n",
    "    index += 1\n",
    "\n",
    "pickle.dump(uid_voc, open(\"uid_voc.pkl\", \"wb\"))\n",
    "pickle.dump(mid_voc, open(\"mid_voc.pkl\", \"wb\"))\n",
    "pickle.dump(cat_voc, open(\"cat_voc.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成负userseq并符合模型输入要求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "import json\n",
    "import random\n",
    "\n",
    "import gzip\n",
    "\n",
    "def load_dict(filename):\n",
    "    F=open(filename,'rb')\n",
    "    return pickle.load(F)\n",
    "\n",
    "\n",
    "def fopen(filename, mode='r'):\n",
    "    if filename.endswith('.gz'):\n",
    "        return gzip.open(filename, mode)\n",
    "    return open(filename, mode)\n",
    "\n",
    "def generator_neg(source,\n",
    "                 uid_voc,\n",
    "                 mid_voc,\n",
    "                 cat_voc,\n",
    "                 maxlen=None,\n",
    "                 skip_empty=False,\n",
    "                 shuffle_each_epoch=False,\n",
    "                 sort_by_length=True,\n",
    "                 minlen=None):\n",
    "    if shuffle_each_epoch:\n",
    "        source_orig = source\n",
    "        source = random.shuffle.main(source_orig, temporary=True)\n",
    "    else:\n",
    "        source = fopen(source, 'r')#数据文件\n",
    "           \n",
    "    source_dicts = []\n",
    "    for source_dict in [uid_voc, mid_voc, cat_voc]:\n",
    "        source_dicts.append(load_dict(source_dict))#文件\n",
    "        \n",
    "\n",
    "    f_meta = open(\"item-info\", \"r\")\n",
    "    meta_map = {}#asin-category\n",
    "    for line in f_meta:\n",
    "        arr = line.strip().split(\"\\t\")\n",
    "        if arr[0] not in meta_map:\n",
    "            meta_map[arr[0]] = arr[1]\n",
    "                \n",
    "    meta_id_map ={}#asin-category\n",
    "    for key in meta_map:#asin\n",
    "        val = meta_map[key]#category\n",
    "        if key in source_dicts[1]:#mid_voc:asin-index\n",
    "             mid_idx = source_dicts[1][key]#asin_index\n",
    "        else:\n",
    "            mid_idx = 0\n",
    "        if val in source_dicts[2]:#cat_voc\n",
    "            cat_idx = source_dicts[2][val]#category_index\n",
    "        else:\n",
    "            cat_idx = 0\n",
    "        meta_id_map[mid_idx] = cat_idx#asin_index----category_index\n",
    "\n",
    "    f_review = open(\"reviews-info\", \"r\")\n",
    "    mid_list_for_random = []\n",
    "    for line in f_review:\n",
    "        arr = line.strip().split(\"\\t\")#userId,asin,rating,time\n",
    "        tmp_idx = 0\n",
    "        if arr[1] in source_dicts[1]:#mid_voc:asin-index\n",
    "            tmp_idx = source_dicts[1][arr[1]]\n",
    "        mid_list_for_random.append(tmp_idx)\n",
    "\n",
    "    n_uid = len(source_dicts[0])\n",
    "    n_mid = len(source_dicts[1])\n",
    "    n_cat = len(source_dicts[2])\n",
    "\n",
    "    shuffle = shuffle_each_epoch\n",
    "\n",
    "    end_of_data = False\n",
    "    \n",
    "    feats = []\n",
    "    target = []\n",
    "    \n",
    "    num=0\n",
    "    for line in source:\n",
    "        ss = line.strip(\"\\n\").split(\"\\t\")#strip() 方法用于移除字符串头尾指定的字符（默认为空格）或字符序列\n",
    "\n",
    "        if len(ss)==0:\n",
    "            continue\n",
    "        # sort by  history behavior length\n",
    "\n",
    "        uid = source_dicts[0][ss[1]] if ss[1] in source_dicts[0] else 0\n",
    "        mid = source_dicts[1][ss[2]] if ss[2] in source_dicts[1] else 0\n",
    "        cat = source_dicts[2][ss[3]] if ss[3] in source_dicts[2] else 0\n",
    "\n",
    "        tmp = []\n",
    "        for fea in ss[4].split(\"\u0002\"):\n",
    "            m = source_dicts[1][fea] if fea in source_dicts[1] else 0\n",
    "            tmp.append(m)\n",
    "        mid_list = tmp\n",
    "\n",
    "        tmp1 = []\n",
    "        for fea in ss[5].split(\"\u0002\"):\n",
    "            c = source_dicts[2][fea] if fea in source_dicts[2] else 0\n",
    "            tmp1.append(c)\n",
    "        cat_list = tmp1\n",
    "\n",
    "        lens = len(mid_list)\n",
    "        # read from source file and map to word index\n",
    "\n",
    "        #if len(mid_list) > self.maxlen:\n",
    "        #    continue\n",
    "        if lens==0:#如果用户序列为0，直接跳过\n",
    "            continue\n",
    "        if minlen != None:\n",
    "            if len(mid_list) <= minlen:\n",
    "                continue\n",
    "        if skip_empty and (not mid_list):\n",
    "            continue\n",
    "\n",
    "        #user behavior负样本，序列中的每个行为对应生成一个负样本\n",
    "        noclk_mid_list = []\n",
    "        noclk_cat_list = []\n",
    "        for pos_mid in mid_list:\n",
    "            while True:\n",
    "                noclk_mid_indx = random.randint(0, len(mid_list_for_random)-1)\n",
    "                noclk_mid = mid_list_for_random[noclk_mid_indx]\n",
    "                if noclk_mid != pos_mid:\n",
    "                    noclk_mid_list.append(noclk_mid)\n",
    "                    noclk_cat_list.append(meta_id_map[noclk_mid])\n",
    "                    break\n",
    "\n",
    "        #item behavior 用户id_lens_items_cats\n",
    "        if int(ss[7]) == 0:\n",
    "            continue\n",
    "\n",
    "        item_bhvs_uid_feats = []\n",
    "        item_bhvs_num_feats = []\n",
    "        item_bhvs_id_feats = []\n",
    "        item_bhvs_cat_feats = []\n",
    "        for bhvs in ss[6].split(\";\")[1:]:\n",
    "            arr = bhvs.split(\"_\")\n",
    "            if arr[1].strip() == \"\":\n",
    "                continue\n",
    "\n",
    "            bhv_uid = source_dicts[0][arr[0]] if arr[0] in source_dicts[0] else 0\n",
    "            item_bhvs_uid_feats.append(bhv_uid)\n",
    "\n",
    "            id_tmp_list = []\n",
    "            for fea in arr[1].split(\"\u0002\"):\n",
    "                m = source_dicts[1][fea] if fea in source_dicts[1] else 0\n",
    "                id_tmp_list.append(m)\n",
    "            item_bhvs_id_feats.append(id_tmp_list)\n",
    "\n",
    "            item_bhvs_num_feats.append(len(id_tmp_list))\n",
    "\n",
    "            cat_tmp_list = []\n",
    "            for fea in arr[2].split(\"\u0002\"):\n",
    "                c = source_dicts[2][fea] if fea in source_dicts[2] else 0\n",
    "                cat_tmp_list.append(c)\n",
    "            item_bhvs_cat_feats.append(cat_tmp_list)\n",
    "\n",
    "        feats.append([uid, mid, cat, lens, mid_list, cat_list, noclk_mid_list, noclk_cat_list, item_bhvs_uid_feats,item_bhvs_num_feats,item_bhvs_id_feats,item_bhvs_cat_feats,int(ss[7])])\n",
    "        target.append([float(ss[0]), 1-float(ss[0])])\n",
    "        num+=1\n",
    "    print(\"sample counts\",num) \n",
    "    return feats,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = \"jointed-new-userseq-itemseq\"\n",
    "uid_voc = \"uid_voc.pkl\"\n",
    "mid_voc = \"mid_voc.pkl\"\n",
    "cat_voc = \"cat_voc.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats,target = generator_neg(fin, uid_voc, mid_voc, cat_voc, shuffle_each_epoch=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def getlist(features,labels):\n",
    "    data = []\n",
    "    for s,t in zip(features,labels):\n",
    "        uid = s[0]\n",
    "        mid = s[1]\n",
    "        cat = s[2]\n",
    "        histlen = s[3]\n",
    "        mid_list = s[4]\n",
    "        cat_list = s[5]\n",
    "        mid_neg_list = s[6]\n",
    "        cat_neg_list = s[7]\n",
    "        \n",
    "        item_uid_list = s[8]\n",
    "        item_midlen_list = s[9]\n",
    "        item_mid_list = s[10]\n",
    "        item_cat_list = s[11]\n",
    "        item_histlen = s[12]\n",
    "\n",
    "        label = int(t[0])\n",
    "        data.append((uid, mid, cat, histlen, mid_list, cat_list, mid_neg_list, cat_neg_list, item_uid_list, item_midlen_list,\\\n",
    "                     item_mid_list, item_cat_list, item_histlen, label))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalist = getlist(feats,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datalist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 按照8:2划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_num =int(len(datalist)*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdatalist=datalist[:cut_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tedatalist=datalist[cut_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(trdatalist)\n",
    "random.shuffle(tedatalist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=open(\"uid_voc.pkl\",'rb')\n",
    "f2=open(\"mid_voc.pkl\",'rb')\n",
    "f3=open(\"cat_voc.pkl\",'rb')\n",
    "user = pickle.load(f1)\n",
    "item = pickle.load(f2)\n",
    "cate = pickle.load(f3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count, item_count, cate_count=\\\n",
    "    len(user), len(item), len(cate)\n",
    "print('user_count: %d\\titem_count: %d\\tcate_count: %d' %\n",
    "      (user_count, item_count, cate_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 第一次保存完整的处理后的数据结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Grocery1.pkl', 'wb') as f:\n",
    "    pickle.dump(trdatalist, f, pickle.HIGHEST_PROTOCOL)#整型，最高协议版本\n",
    "    pickle.dump(tedatalist, f, pickle.HIGHEST_PROTOCOL)#整型，最高协议版本\n",
    "    pickle.dump((user_count, item_count, cate_count), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 删除相似用户中的用户本身"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Grocery1.pkl', 'rb') as f: \n",
    "    train_data = pickle.load(f)\n",
    "    test_data = pickle.load(f)\n",
    "    user_count, item_count, cate_count = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "trdata = copy.deepcopy(train_data)\n",
    "tedata = copy.deepcopy(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(uid, mid, cat, histlen, mid_list, cat_list, mid_neg_list, cat_neg_list, item_uid_list, item_midlen_list,\\\n",
    "                     #item_mid_list, item_cat_list, item_histlen, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutself(data):\n",
    "    #random.shuffle(data)\n",
    "    ts=data\n",
    "    counts = len(ts)\n",
    "    \n",
    "    datacself = []\n",
    "    for s in ts:\n",
    "        uid = s[0]\n",
    "        mid = s[1]\n",
    "        cat = s[2]\n",
    "        histlen = s[3]\n",
    "        mid_list = s[4]\n",
    "        cat_list = s[5]\n",
    "        mid_neg_list = s[6]\n",
    "        cat_neg_list = s[7]\n",
    "        \n",
    "        item_uid_list = s[8]\n",
    "        item_midlen_list = s[9]\n",
    "        item_mid_list = s[10]\n",
    "        item_cat_list = s[11]\n",
    "        item_histlen = s[12]\n",
    "        label = s[13]\n",
    "        \n",
    "        if uid in item_uid_list:\n",
    "            index = item_uid_list.index(uid)\n",
    "            item_uid_list.pop(index)\n",
    "            item_midlen_list.pop(index)\n",
    "            item_mid_list.pop(index)\n",
    "            item_cat_list.pop(index)\n",
    "            #item_histlen -= 1\n",
    "            \n",
    "        if len(item_uid_list)==0:\n",
    "            continue\n",
    "            \n",
    "        datacself.append((uid, mid, cat, histlen, mid_list, cat_list, mid_neg_list, cat_neg_list, item_uid_list, item_midlen_list,\\\n",
    "                     item_mid_list, item_cat_list, len(item_uid_list), label))\n",
    "    \n",
    "    return datacself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdata_cs = cutself(trdata)\n",
    "tedata_cs = cutself(tedata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Grocery2.pkl', 'wb') as f:\n",
    "    pickle.dump(trdata_cs, f, pickle.HIGHEST_PROTOCOL)#整型，最高协议版本\n",
    "    pickle.dump(tedata_cs, f, pickle.HIGHEST_PROTOCOL)#整型，最高协议版本\n",
    "    pickle.dump((user_count, item_count, cate_count), f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 细节处理\n",
    "    For these three datasets, we set the maximum sequence length to 60 and the maximum number of similar users in a group to 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Grocery2.pkl', 'rb') as f: \n",
    "    train_data = pickle.load(f)\n",
    "    test_data = pickle.load(f)\n",
    "    user_count, item_count, cate_count = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata_fixedlen(data):\n",
    "    #random.shuffle(data)\n",
    "    ts=data\n",
    "    counts = len(ts)\n",
    "    \n",
    "    datacself = []\n",
    "    for s in ts:\n",
    "        uid = s[0]\n",
    "        mid = s[1]\n",
    "        cat = s[2]\n",
    "        \n",
    "        mid_list = s[4][-60:]\n",
    "        cat_list = s[5][-60:]\n",
    "        mid_neg_list = s[6][-60:]\n",
    "        cat_neg_list = s[7][-60:]\n",
    "        histlen = len(mid_list)\n",
    "        \n",
    "        item_uid_list = s[8][:50]\n",
    "        item_midlen_list = []\n",
    "        \n",
    "        item_mid_list = []\n",
    "        for fea in s[10]:\n",
    "            a = fea[-60:]\n",
    "            item_mid_list.append(a)\n",
    "            item_midlen_list.append(len(a))\n",
    "        item_mid_list=item_mid_list[:50]\n",
    "        item_midlen_list=item_midlen_list[:50]\n",
    "        \n",
    "        item_cat_list = []\n",
    "        for fea in s[11]:\n",
    "            a = fea[-60:]\n",
    "            item_cat_list.append(a)\n",
    "        item_cat_list=item_cat_list[:50]\n",
    "        \n",
    "        #item_histlen = len(item_mid_list)\n",
    "        label = s[13]\n",
    "            \n",
    "        datacself.append((uid, mid, cat, histlen, mid_list, cat_list, mid_neg_list, cat_neg_list, item_uid_list, item_midlen_list,\\\n",
    "                     item_mid_list, item_cat_list, len(item_uid_list), label))\n",
    "    \n",
    "    return datacself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_re = getdata_fixedlen(train_data)\n",
    "test_re = getdata_fixedlen(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('Groceryf.pkl', 'wb') as f:#60,50\n",
    "    pickle.dump(train_re, f, pickle.HIGHEST_PROTOCOL)#整型，最高协议版本\n",
    "    pickle.dump(test_re, f, pickle.HIGHEST_PROTOCOL)#整型，最高协议版本\n",
    "    pickle.dump((user_count, item_count, cate_count), f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
